{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-10T07:34:03.048391Z",
     "start_time": "2023-11-10T07:34:02.634087Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/micahtilton/Documents/GitHub/hacc-askus/question-answer/venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import trafilatura\n",
    "from helpers.openai_helper import split_by_tokens, get_embedding\n",
    "import json\n",
    "from helpers.scrape_html import clean_text\n",
    "import io\n",
    "import PyPDF2\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cleaning URLS"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2ea1e208499897e"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def clean_url(url):\n",
    "  url = url.strip()\n",
    "  if url.endswith(\"/\"):\n",
    "    url = url[:-1]\n",
    "  return re.sub(r'/#.*$', '', url)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T07:34:04.530159Z",
     "start_time": "2023-11-10T07:34:04.522246Z"
    }
   },
   "id": "cc2969ea056c1185"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "def clean_urls(urls):\n",
    "  return [clean_url(url) for url in urls if url.strip()]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T08:18:39.596282Z",
     "start_time": "2023-11-10T08:18:39.580794Z"
    }
   },
   "id": "38e801e9d7969014"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "def clean_urls_file(fp):\n",
    "  with open(fp, \"r+\") as f:\n",
    "    cleaned_urls = clean_urls(f.read().split(\"\\n\"))\n",
    "    cleaned_urls = list(set(cleaned_urls))\n",
    "    cleaned_urls.sort()\n",
    "  with open(fp, \"w+\") as f:\n",
    "    f.write(\"  \\n\".join(cleaned_urls))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T08:18:40.700854Z",
     "start_time": "2023-11-10T08:18:40.688419Z"
    }
   },
   "id": "e420c9d8ede30740"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "def add_urls_to_file(fp, urls_set):\n",
    "  with open(fp, \"r\") as f:\n",
    "    cleaned_urls = clean_urls(f.read().split(\"\\n\"))\n",
    "    cleaned_urls = set(cleaned_urls)\n",
    "    cleaned_urls = list(cleaned_urls.union(urls_set))\n",
    "    cleaned_urls.sort()\n",
    "  with open(fp, \"w\") as f:\n",
    "    f.write(\"  \\n\".join(cleaned_urls))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T08:18:41.140478Z",
     "start_time": "2023-11-10T08:18:41.133484Z"
    }
   },
   "id": "afe1bb46c4a2d572"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Getting Relevant Info"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6444d2070b08957d"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def read_pdf(content):\n",
    "  pdf = io.BytesIO(content)\n",
    "  reader = PyPDF2.PdfReader(pdf)\n",
    "  pages = reader.pages\n",
    "  text = \"\\n\".join([page.extract_text() for page in pages])\n",
    "  return text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T07:34:07.806300Z",
     "start_time": "2023-11-10T07:34:07.795231Z"
    }
   },
   "id": "b565a0c96e1ace3"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def get_relevant_info(url):\n",
    "  request = requests.get(url)\n",
    "  \n",
    "  if url.endswith(\".pdf\"):\n",
    "    read_pdf(request.content)\n",
    "\n",
    "  relevant_info = trafilatura.extract(request.content, include_links=True)\n",
    "  \n",
    "  return clean_text(relevant_info) if relevant_info else \"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T07:45:21.650476Z",
     "start_time": "2023-11-10T07:45:21.635689Z"
    }
   },
   "id": "e120cf3989cc28d5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Web Crawler"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "662b7359fa8945ea"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def crawl_into_url(urls, allowed_domains=None, restricted_domains=None, seen_urls_fp =\"\", max_depth = 2):\n",
    "  urls = [clean_url(url) for url in urls]\n",
    "  \n",
    "  if allowed_domains is None:\n",
    "    allowed_domains = []\n",
    "  \n",
    "  if restricted_domains is None:\n",
    "    restricted_domains = []\n",
    "\n",
    "  urls_seen = set()\n",
    "  \n",
    "  if seen_urls_fp != \"\":\n",
    "    try:\n",
    "      with open(seen_urls_fp, \"r\") as f:\n",
    "        urls_seen = set([clean_url(url) for url in f.read().split(\"\\n\")])\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "  \n",
    "  all_urls = []\n",
    "  \n",
    "  urls_to_visit = urls\n",
    "  urls_to_visit_next = []\n",
    "  \n",
    "  depth = 0\n",
    "  while depth < max_depth:\n",
    "    for url in urls_to_visit:\n",
    "      if url in urls_seen:\n",
    "        print(\"SEEN    : \" + url)\n",
    "        continue\n",
    "\n",
    "      print(\"VISITING: \" + url)\n",
    "      urls_seen.add(url)\n",
    "      all_urls.append(url)\n",
    "      r = requests.get(url) \n",
    "      soup = BeautifulSoup(r.text)\n",
    "      found_urls = list(map(lambda x: x['href'], soup.find_all('a', href=True)))\n",
    "      \n",
    "      for found_url in found_urls:\n",
    "        if not found_url.startswith(\"https://\"):\n",
    "          continue\n",
    "        \n",
    "        if allowed_domains:\n",
    "         if not any(map(lambda domain: found_url.startswith(domain), allowed_domains)):\n",
    "           continue\n",
    "        \n",
    "        if restricted_domains:\n",
    "          if any(map(lambda domain: found_url.startswith(domain), restricted_domains)):\n",
    "            continue\n",
    "                         \n",
    "        urls_to_visit_next.append(clean_url(found_url))\n",
    "        \n",
    "    urls_to_visit = urls_to_visit_next\n",
    "    urls_to_visit_next = []\n",
    "    depth = depth + 1\n",
    "      \n",
    "  return all_urls"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T07:59:55.941271Z",
     "start_time": "2023-11-10T07:59:55.934526Z"
    }
   },
   "id": "7b90d7778123fcb4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ed1c99e2b557a43"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def embed_urls(fp, urls, seen_urls_fp=\"\"):  \n",
    "  embedding_arr = []\n",
    "  \n",
    "  seen_urls = set()\n",
    "  \n",
    "  if seen_urls_fp:\n",
    "    with open(seen_urls_fp, \"r\") as f:\n",
    "      seen_urls = set(clean_urls(f.read().split(\"\\n\")))\n",
    "  \n",
    "  for url in urls:\n",
    "    relevant_info = get_relevant_info(url)\n",
    "    \n",
    "    if relevant_info == \"\":\n",
    "      print(\"NO DATA  : \" + url)\n",
    "      continue\n",
    "    elif url in seen_urls:\n",
    "      print(\"SEEN     : \" + url)\n",
    "      continue\n",
    "    \n",
    "    print(\"EMBEDDING: \" + url)\n",
    "    text_splits = split_by_tokens(clean_text(relevant_info))\n",
    "    for text_split in text_splits:\n",
    "      embedding = get_embedding(text_split)\n",
    "      embedding_arr.append({\n",
    "        \"source\": url,\n",
    "        \"embedding\": embedding,\n",
    "        \"text\": text_split,\n",
    "      })\n",
    "  \n",
    "  with open(fp, \"w+\") as f:\n",
    "    json.dump(embedding_arr, f)\n",
    "  add_urls_to_file(seen_urls_fp, seen_urls)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T08:08:44.975601Z",
     "start_time": "2023-11-10T08:08:44.965847Z"
    }
   },
   "id": "3cb4d213395f72bc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Usage\n",
    "### This is how we generated the embedding data for the crawled websites"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31b3a8150089e6c1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "urls_to_embed = crawl_into_url(\n",
    "  [\"https://www.hawaii.edu/its/services\", \"https://www.hawaii.edu/its/services\", \"https://www.hawaii.edu/infosec/policies/\"], \n",
    "  allowed_domains=[\"https://www.hawaii.edu\"], \n",
    "  restricted_domains=[\"https://www.hawaii.edu/askus\"], \n",
    "  seen_urls_fp=\"./data/seen_urls.txt\", \n",
    "  max_depth=3\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d14356a54ed0e319"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embed_urls(\"./data/embeddings/embedding-data.json\", urls_to_embed, seen_urls_fp=\"./data/seen_urls.txt\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b9b24ada6bc62b9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
